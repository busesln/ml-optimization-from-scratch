# ML Optimization & Regression Analysis from Scratch

This repository contains a custom implementation of **Linear Regression** and various **Optimization Algorithms** (Gradient Descent, Adam, RMSprop) built entirely from scratch using Python and NumPy. 

**ğŸš« No High-Level ML Frameworks (like Scikit-Learn) were used for the core logic.** The goal of this project is to demonstrate a deep understanding of the mathematical underpinnings of machine learning models.

## ğŸš€ Features

* **Manual Implementation:** Core algorithms are written using matrix multiplication and calculus concepts.
* **Custom Optimizers:**
    * Batch Gradient Descent
    * Stochastic Gradient Descent (SGD)
    * Adam & RMSprop
* **Visualization:** Detailed plots of the cost function landscape and regression fit using Matplotlib.

## ğŸ› ï¸ Technologies Used

* **Language:** Python
* **Libraries:** NumPy (Matrix ops), Matplotlib (Visualization), Pandas (Data handling)

## ğŸ“Š Visualizations

*(Buraya projenin Ã§Ä±ktÄ±sÄ± olan o regresyon Ã§izgisi grafiÄŸinin veya loss grafiÄŸinin ekran gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ "Screenshots" klasÃ¶rÃ¼ne atÄ±p linkle. Ã–rnek:)*
![Regression Plot](path/to/your/image.png)

## ğŸ§  What I Learned

* Deriving gradients for Mean Squared Error (MSE).
* Understanding the impact of Learning Rate on convergence.
* The internal mechanics of Adam and RMSprop optimizers.

## ğŸ’¿ Usage

```bash
git clone [https://github.com/busesln/ml-optimization-from-scratch.git](https://github.com/busesln/ml-optimization-from-scratch.git)
cd ml-optimization-from-scratch
python main.py
